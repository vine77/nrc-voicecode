Todo list for VoiceCode
=======================

To do before 1st release
========================


Link with Emacs through vr-mode
-------------------------------

- VR-mode starts a new instance of the "server" (at this point,
  VR.exe) every time it connects to DNS. This is probably because
  VR.exe is not multithreaded and can only handle one connection at a
  time (don't want to change that, cause multithreading in C++ is probably 
  a nightmare). 

  This is a problem for VC because then different instances of Emacs will
  end up sharing different symbols dictionnaries.

  Possible solution
  *****************
  
  Let Emacs start a new instance of VC server. But the server will try to bind
  to its two sockets. If one of the binds fails, it means that there is already
  an instance of VC server running (there may be a more direct way of finding
  this out, but I doubt it because the name of the process running VC server
  will probably be 'python').

  If VC server finds out it is already running, it will exit, otherwise it will
  start listening on the two ports. 

  But before it does any of those two things, it will output on STDOUT
  the message 'listening portNum'. Emacs needs this because every
  instance of VR.exe listens on a different port number (that it
  selects).

  Code below illustrates how VC server could check for busy port.

import socket, time

ss_list = []

for ii in range(3):

    ss = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    
    portNum = 45771
    while 1:
        try:
            print '-- checking availability portNum=%s' % portNum
            ss.bind('127.0.0.1', portNum)
            ss_list.append(ss)
            print '-- portNum=%s was available' % portNum
            break
        except socket.error, why:
            # could analyze why here
            print '-- failed, why=%s' % why
            portNum = portNum + 1

    time.sleep(1)

  
- Make VC server use the same port for the two connections.

  This is what VR-mode does. Using the same port doesn't mean you use
  the same *connection*! You can have two connections on the same
  port.

  In the handshake for both types of connections, the first thing that
  the client does is to let the server know whether it's asking for a
  VC_LISTEN or VC_TALK connection.

  An advantage of this is that for SSH, need only to tunnel one port


Communication protocol
----------------------


- In tcp_server.py, use postMessageCallback instead of explicit events
  caught in the mean thread.

- Add a dummy authentification sequence in the handshake part of the
  protocol. For now, this will be ignored, but later on we (i.e. Eric
  Johanssen ;-) can add a true authentification implementation.

- Make sure that David invokes AppState.recog_start() in the global
  got_begin() callback.

- Implement AppState.recog_indicator so it does something to Dragon's
  recognition indicator.

- Is SourceBuffIndent and EdSimIndent still needed?

- Implement an XML based messaging protocol. It will be easier to
  handle at the editor end than Alain's protocol, because there are
  tons of XML libraries in different langauges out there.

- For better efficiency, SourceBuffCached.get_text(start, end) could
  retrieve only the text between *start* and *end* (*text_region*),
  and cache the 3ple *(start, end, text_region)*. This would allow certain 
  commands that only access part of the text to not have to download and cache
  the whole and then only use a small part of it.

Regression testing
------------------

When -s 0 (i.e. user doesn't plan to switch between windows), use
local non-exclusive grammars instead of global exclusive ones. This
more closely approximates the way that VoiceCode is used in actual
production setting.


BUG in setmic
-------------

Can't turn mic on after turned it off once. Ex:

python gui_sim.py -s
setmic('on')
I say: "for" -> types "for ^:\n"
setmic('off')
setmic('on')
I say: "for" -> doesn't type anything


BUG in back indent
------------------

When cursor is at end of current line and say 'back indent', then it just deletes the CR character (but it seems to leave the LF character there).

This doesn't happen when cursor is not at the end of the current line.



BUG in gui_sim
--------------

> > The problem happens when the phrases are too long. If I issue the
> above
> > say command twice in a row, the server busy message appears, and the
> > results are incomplete. This behavior is reproducible. Even worse,
> sometimes
> > the application hangs after a long phrase and I need to reboot.
> 
> Can you send me a recipe for reproducing the bug and I'll try it on my
> end.

After reboot I do the following:

1) startup gui_sim

2) paste the following command into the command line and wait until the
result is displayed
say(['boost', 'slash', 'graph', 'slash', 'graph', 'traits', '.\\dot', 'H.',
'P.', 'P.'])


Performance
-----------

Command translation seems very slow. Profile it to figure out what is
making it so slow.

Also, startup is very slow. Why?


BUGS: Regression test and pickle
--------------------------

David wrote:

During one of my tests from the command line, I did
say(['remove','test']) and the interpreter typed remove_test.  However,
when I ran the mediator again, it had apparently pickled this new symbol,
but it tried to add it as 'remove_test\ ', which caused an exception in
addWord, crashing the mediator.  The only way I was able to get the
mediator to run again was to delete the symdict.pkl file.  Some
of the problems getting the regression test to come out the same way may
therefore be due to the change in the set of pickled symbols.

Q1: Why did the mediator try to add the symbol as 'remove_test\ '?

Q2: Why did the regression test use the pickle file? 


BUG
---

When I tried to test a variant of Wieger's > example.  I said 'value
greater than 5' and got value_greater_than_10, both with say(...) and
actually dictating it.

Then, when I exited and tried to run gui_sim again, it crashed
while trying to read the known symbols in from the pickle file.  I
unpickled the file manually without adding the words to natspeak, to see 
what the problem was.  It appears that SymDict.symbol_info contains an entry
x = symbol_info['value_greater_than_5']  
with 

x.spoken_forms = ['value greater than 5', 'v', 'a', 'l', 'u', 'e', ' ', 
'g', 'r', 't', '-', 'h', 'n', '5']

Thus SymDict.init_from_file tries to add words

value_greater_than_5\value greater than 5

but also

value_greater_than_5\v
value_greater_than_5\a

etc.

When it gets to the word with spoken form ' ', which is not a legal
spoken form, natlink.addWord throws an exception natlink.InvalidWord.
Since init_from_file doesn't catch this exception, gui_sym.py crashes.

This may be the cause of Wieger's inability to run the simulator twice
in a row.

The addWord behaviour seems to be different when the new symbol is
initially added, since that doesn't trigger the error.


Note that the SymDict is pickled on exit even if you don't save your
speech files.  This should probably be changed.

David

P.S.  For robustness, init_from_file should catch this exception and
print a warning and/or delete the bad entry, instead of crashing, though
this obviously doesn't solve the underlying problem of the bad spoken
forms but at least makes it possible to continue testing without having
to be familiar enough with the mediator code to know to find and delete
the pickle file.


BUG: Code indentation back indentation
--------------------------------------

BUG #1: Doesn't work on first line of a buffer because the line is not preceded by a '\n'. The fix is to modify decr_indent_level and incr_indent_level so that they check if the first line is in the range and indent it before calling indent_by_spaces. 

Note: we don't want indent_by_spaces to do a substitution on '(^\n)'
because it is sometimes used to indent code that's in the middle of a
line and that would result in the addition of spaces before that code.

BUG #2: In gui_sim.py, 'back indent' and 'indent' don't work properly. It se

Keep doing that
---------------

Implement a keep "doing that" suffix command that will keep doing the
repeatable command at a specified time interval (a new attribute of
repeatable). Then "Stop", "Reverse", "faster", "slower" would affect
the command being repeated.

Need to implement a Mode object. This will remember what mode the mediator is in. "Keep doing that" would make the mode be a ModeKeepRepeating object. Context objects (like ContAny()) would check to make sure that the mode is "None". There would be some context objects like ContModeKeepRepeating that would check that the mode is of a particular class. 



BUGS in pseudo python
---------------------

'back slash cap charlie' don't work in regression test.

uncomment python symbols in py_std_sym.py


Division of labour between EdSim, AppState and SourceBuff
---------------------------------------------------------

print_buff: and its supporting methods (number_line etc...) should be
implemented at the level of SourceBuff, not SourceBuffEdSim (could
be useful for debugging purposes).


"Not what I said" correction
----------------------------

-- Alain:

Need to be able to correct errors where an utterance was translated in
the wrong way.

The table below shows all the possible kinds of errors. The first
column gives the type of utterance the user meant. The 2nd column
gives the type of utterance it was interpreted to be. The third column
descirbes circumstances when this may happen.

User	Interpreter
Meant   understood      Happens when
-----   -----------	------------

text	CSC		if user didn't turn off interpretation when he 
			should have

	LSA		if user didn't turn off interpretation when he 
			should have

	sym		if user didn't turn off interpretation when he 
			should have

sym	CSC		if spoken symbol is same or substring of spoken CSC

	LSA		if spoken symbol is same or substring of spoken LSA

	text		if interpretation was turned off when it shouldn't have

        wrong sym       happens if two symbols have same spoken form and
                        interpreter chose the wrong one

LSA	CSC		if spoken LSA is same or substring of spoken CSC

	sym		if spoken LSA is STRICT SUBSTRING of spoken symbol 
			(otherwise, the LSA will take precedence).

	text		if interpretation was turned off when it shouldn't have

CSC	LSA		if spoken CSC is STRICT SUBSTRING of spoken LSA 
			(otherwise, the CSC will take precedence)

	sym		if spoken symbol is STRICT SUBSTRING of spoken LSA 
			(otherwise, the CSC will take precedence)

	text		if interpretation was turned off when it shouldn't have


A full fledge correction dialog might allow the user to select a
misinterpreted portion of the utterance which was meant as a SINGLE
symbol, CSC, LSA or textual phrase. In other words, if two consecutive
LSA, CSC, symbols or phrase were misinterpreted, the user can't
correct both at the same time. That's because things become very
complicated to display (need to show the possible interpretations of
both, with possibly different separation points of the utteranace for
where the first command ends and the second one starts).

So for example, if the user said:

"items for sale equals 0" meaning "itemsForSale = 0" and the "for" got
interpreted as a for loop, the user would say: "correct items for
sale".

He would then get a dialog box which among other things, would have 3 buttons:

"meant symbol"
"meant text"
"meant command"

The user would say: "meant symbol", at which point the system would
display all the possible ways that "items for sale" might be
interpreted as a symbol. When the user selects one, the system undo
and redo the interpretation to take that into account (more on how
that would be done later).

An other example, suppose the user meant "items for (sale = 0^=0; <= ;
++){\n}" (i.e. "for" really meant a for loop, doesn't make syntactic
sense but nevermind ) but it got recognised as "itemsForSale = 0". The
user would say: "correct for" and then "meant command". The system
would then display the docstring of all the LSAs and CSCs that "for"
might translate to. When the user picks one, the system would again undo and redo the interpretation accordingly.

If the user meant the text "items for sales equals 0", then he would
say: "correct from items up to 0" and then say: "meant text".

The undo/redo gets a bit complicated, if the misinterpretation caused an
incorrect segmentation of the utterance. In the above example where a
for loop gets interpreted as being part of a symbol, the utterance was
segmented as:

'items for sale' | 'equals' | '0'

but the correct segmentation is:

'items' | 'for' | 'sale' | 'equals' | '0'

So this means that we need to undo the interpretation up to before
'items', eventhough it's not part of what the user selected when he
said "correct for". Moreover, when we reinterpret 'items for sale
etc...', the interpreter will just repeat the mistake it did.

So the way to go is to undo every thing up to the end of the last
correctly interpreted part of the utterance (just before 'items' in
this case), then remove the portion of the utterance selected by the
user for correction as well as everything that followed it (in this
case: 'for sale equals 0') from the utterance before reinterpreting
it. This will force a segmentation point at the start of the portion
selected by the user (in this case: 'for'). Then, we execute whatever
action/symbol/text/LSA was selected by the user in the correction
dialog (in this case: CSC 'for') and finally, reinterpret everything
that followed the portion of the utterance selected by the user (in
this case: 'sale equals 0').

Note that when reinterpreting the misinterpreted portion that preceded
the portion selected by the user, there may be problems in cases where
the meaning of reinterpreted portion is different when it is at the
end of an utterance than when it is in the middle of one (can't think
of an example right now). But if that's the case, we can insert some
dummy part of speech (e.g. '@#$$%$%@#') whose interpretation is to do
absolutely nothing. This utterance shouldn't be logged in the command
history. Bit of a hack.




Turning off translation
-----------------------

-- Alain:

Need to support 3 modes:

coding mode

    everything gets interpreted

string mode

    only limited set of CSCs and LSAs are interpreted. Symbols aren't.
    All global LSAs are interpreted. Navigational CSCs and non-coding
    ones (e.g. open file)' are enabled, especially, CSCs for jumping out of
    a string.

comment mode

    only limited set of CSCs and LSAs are interpreted. Symbols aren't.
    All global LSAs are interpreted. Navigational CSCs and non-coding
    ones (e.g. open file)' are enabled, especially those for creating new
    comment lines.

dictation
    
    only global LSAs are interpreted. Everything else is typed as dictated
    text.

Some commands would automatically change the system's mode. For
example, 'between quotes' would put the system in 'string' mode. CSC
'jump out' or 'out of quote' would put it in coding mode. Etc...

One way to support this would be for AppState to have an attribute
cur_mode in ('code', 'string', 'comment', 'dictation'). Method
chop_symbol would only chop a symbol if mode was 'code'. Method
chop_LSA would not chop a non-global LSA, unless mode was
'code'. Method chop_CSC would always chop the CSC because the context
of the CSC itself would determine in what modes they are enabled.

There would be a class ContModes with attribute mode_list, which would
list the modes in which the context applies. ContLanguages would be
descendants of ContModes with mode_list = ['code'].

There would be CSCs 'coding mode', 'string mode', 'comment mode' and
'dictation mode' which would always be active and turn the mode to the
right value.

Eventually, those modes will be determined automatically based on the
code context.

Maybe string and comment modes are really the same thing?


Symbol compilation
------------------

-- Alain

Compiling py_std_symbols.py is very slow. This is a real problem with
regression testing since we regenerate the mediator object before
every test.

Why is that? If it can't be
made faster, maybe we should not recompile it everytime we start
Mediator, but only if it has changed since the last time compiled.

Actually, the problem is the time spent adding words to the
vocabulary. Also, checking if a word exists in the vocabulary is slow
(but much less so than adding words). So one fix is to change the
std_lib recompilation method so that it always recompiles the symbols,
but only updates the sr vocabulary if a flag
sr_voc_was_cleansed=1. That way, the regression testing procedure
could clean the SymDict (but not the the SR vocabulary) before every
test, without having to update the SR vocabulary every time.

Note that the reason we clean the SymDict before every test in the
regression testing procedure is that we want to avoid interaction
effects between tests. For example, it could be that a test A succeeds
only if test B was done before it. By starting afresh for every test
we avoid that. If we don't clean the SR vocabulary between tests,
there is a danger of interaction effects. But that doesn't to be a
real problem. The only possible interaction is that test A succeeds
only on account that B has added a symbol to the voc (which A itself
doesn't add). But in such a case, the problem would likely be visible
because A wouldn't have that symbol in its SymDict and therefore the
mediator would not recognise it as a known symbol (even though the SR
system did recognise it).


Locked SymDict
--------------

-- Alain

Because the SymDict is persistent, we have to make sure that the
SymDict used by all the MediatorObject instances are in sync. Otherwise, they might undo each other's changes to the pickled SymDict.

   Solution: when creating a MediatorObject, the Mediator server would
   pass it a central SymDict instance. This SymDict instance would
   have a lock, which means that only one MediatorObject instance at a
   time could update it.


Symbol compilation
------------------

Whenever a new file is open, its symbols should be compiled. This will
be useful in contexts where source files are not on the same machine
as the mediator (currently, compile_symbols requires that the files
given as arguments be accessible from the mediator).


Test thorgoughly
----------------


Documentation
-------------

Installation, user, developper


Emacs mediator link
-------------------

Must be implemented


Mediator based CachePad
-----------------------

Jonathan is working on it, but not absolutely necessary for 1st release.


Error correction
----------------

Can't do without it. David Fox is working on it.


Basic LSAs and CSCs for a few languages
---------------------------------------

Just enought to entice people into writing more.

Python, C and what else?


getWordInfo
-----------

-- Alain: this is not absolutely necessary, but it's so easy to do:
   might as well do it.

In sr_interface.getWordInfo, if no flag is specified, should use
flag = 1 (i.e. look in backup dictionary).


Select Pseudo-code
------------------

-- Alain

  Should extend the Select grammar so that it allows commands like:

  go (before|after)* (previous|next)* X this would position the cursor
     before or after the previous or next occurence of X. If
     (before|after) is not specied, it would default to say after. If
     (previous|next) is not specified, it would go to the closest one.

  select from (before|after)* (previous|next)* X through (before|after)*
     (previous|next)* Y selects code from X up to Y. Both of X, Y can
     be optionally qualified as above by (previous|next) and
     (before|after).


Interpreter
-----------

Should it take into account the word property (space adding?) or is
that done automatically by VDct?

If we start using word properties, then we don't need the hack where
addWord adds redundant spoken form for words that start/end with
blanks.


Symbol compilation
------------------

-- Alain

Should add symbols with word info such that a blank space is inserted after it.


add_lsa
-------

-- Alain

Right now, there is no way to edit the word info with add_lsa. In
particular, this means you can't say whether there should be spaces
before or after an LSA.

Should modify add_lsa so it allows to specify the word info (full word
info or just spacing?)


Can wait after 1st release
==========================


CSCs
----

-- Alain

The spoken form of CSCs should not be a phrase. For example, the CSC
"for loop" should have spoken form something like "*for-loop*". This
would have two advantages:

a) The interpreter's job would be made easier, because any word that's
not of the form "*XYZ*" or "ABC\XYZ" is considered to be part of a
natural language symbol. 

ACTUALLY, THAT'S NOT TRUE BECAUSE NATSPEAK
MAY INSIST ON SENDING US THE UTTERANCE AS DICTATED TEXT INSTEAD OF A
SINGLE WRITTEN\\SPOKEN WORD.

b) If user really meant the phrase "for loop", not the CSC, he could
use NatSpeak correction dialog and select "for loop" from the
selection box, instead of "*for-loop*". One problem here though is
that the interpreter will then ask if he wants to create a new symbol
for_loop, which would be a bit of a pain.

THAT'S NOT TRUE EITHER BECAUSE THE MEDIATOR WOULD HAVE TO TRANSLATE
THE SENTENCE IN CASE NATSPEAK INSISTED ON SENDING US THE UTTERANCE AS
RECOGNISED TEXT INSTEAD OF A SINGLE WRITTEN\SPOKEN WORD.

Note that even if we eventually allow the spoken form of CSCs to be
simple grammar rules like: "goto <char> on <num>", we can generate voc
entries "*goto*", *on*, *1*, *2*, etc... So if the user says "goto
semicolon on 5", Natspeak would recognise "*goto* *semicolon* *on*
*5*", which makes it clear that all those words are part of a
command. This may become very confusing (e.g. there would be 3 voc
entries for semicolon: "semicolon", ";\semicolon", "*semicolon*".

Hum... need to think some more about this.


"What can I say" generator
--------------------------

A command that would generate HTML files that describe what can be
said for different languages in different contexts.

addWord
-------

-- Alain (note: this may not be necessary if we use wordinfo to decide
   if a word should have leading/trailing blanks. In which case, words
   never need to have leading/trailing blanksin their written forms).

Joel Gould claims that Select XYZ works as long as the written form of
XYZ doesn't have leading/trailing spaces. So addWord doesn't have to
add redundant vocabulary entry for words whose written form only
contains internal spaces. TEST THE CLAIM BEFORE TRYING TO IMPLEMENT
THIS.


-- Alain

   Should be able to qualify a previous Select command. For example, could say:

   - say "Select X"
   - pause
   - if the system didn't select the right one, say "previous one" or 
     "next one" to select the previous or next instance of X

   Other examples of qualifications would be:

   - "select from X" [pause] "up to Y" 
	can be useful if X and/or Y are
        long to say and/or you realise in mid command that you don't
        really know where you want the selection to end

   - "select from X up to Y" [pause] "extend left"
        useful if didnt select from right instance of X but ended at the right
        instance of Y

   This could be implemented as follows:

   Every time a SelectXYZ command is recognised, the system would log
   an entry in the command history describing the arguments of that
   command.

   There would be a context class ContSelectXYZ that would apply
   whenever the last CSC in the command history was a SelectXYZ command.

   There would be CSCs with ContSelectXYZ as their context and the
   appropriate action methods (e.g. change the begining/end of the
   selection).

   This is a specialised example of command qualification which is described 
   below


Command qualification
---------------------

It woudl be nice if a command could qualify a previous command. For example, if you say: 

"page down" [pause] "keep doing that" OR
"page down" [pause] "again 2 times"

the "keep doing that" part means keep doing the previous command and "again 2 times" means do the previous command 2 times. An other example is:

"select XYZ" [pause] "previous one"

here "previous one" means select the previous occurence of XYZ.

Note that this kind of qualification should only apply if specific
types of commands were dictated last. For example, "previous one" can
only qualify CSC that are of type *linear_selection* (i.e. selecting
an item from a linear list, where the concept of a previous and next
item makes sense). This is because if I say:

"table at index previous one"

I probably mean something like:

table[prev_one]

Similarly, "keep doing that" and "again 2 times" should only apply for commands of type "repeatable".

One way to implement that is to create Context classes that applie
only if the last CSC in the command history is of a particular type. This also assumes that the Context object is able to find out certain information about what the CSC eventually did. For example, in the example:

"select XYZ [pause] previous one"

the CSC "previous one" needs to know what XYZ was and which occurence
of it it choose. This means that the logging mechanism has to be sophisticated enough to remember this sort of thing.


config.py
---------

-- Alain

CSCmd should have a __setattr__ method that intercepts sets to the
*meanning* attribute. It would check to make sure that each entry in
meanings is OK, i.e. it's a pair with 1st element being a context
instance and the second being a function pointer.

This would address the common mistake where a user adds the following
configuration statement.

add_csc(spoken_forms=['for loop'], meanings=[[ContC, c_simple_for]])

i.e. the user forgets to add () after ContC, which means he is making
the context be the ContC class itself (as opposed to a ContC INSTANCE)


add_lsa
-------

-- Alain

At the moment, whenever you switch language, you need to unload all
the aliases for that language, and then load those for the new
languages. It might be more efficient to only unload/load those
aliases which are not in both languages.

For example, 'not equal to' means the same thing in C and in Python '
!= '. So no need to unload/reload it. On the other hand, 'with key'
exists in both Python and Perl, but doesn't mean the same thing ('[]'
versus '{}'). So in this case, would need to unload/reload.

Need to check first if this optimisation is really needed.

-- Alain

At the moment, LSAs are removed from NatSpeak's vocab everytime you
change language. That means that NatSpeak looses any adaptation it may
have done to learn the usage pattern of those LSAs.

It would be better if LSAs stayed in the vocab all the time (including
after VoiceCode exited), but if NatSpeak recognises an LSA that's not
the right form for the current langauge, the Interpreter will fix it
by finding an LSA with same spoken form but for the current language.


CSCs
----

--- Alain

It would be nice if you could associate an action with different contexts (e.g. for word 'equals', associate action type_equal_sign to either C, python, etc.. contexts).

This could be done by making the contextual meaning to have a list of contexts associated with an action:

acmd = CSCmd(spoken_forms=['equals', 'assigned value', 'is assigned value'], meanings=[[ContPy(), ContC, type_equal_sign])
add_csc(acmd)


An other approach is to create an OR context that could be instantiated by feeding the various ORed contexts to it. This would be syntactically nasty:

acmd = CSCmd(spoken_forms=['equals', 'assigned value', 'is assigned value'], meanings=[[ContOR(terms=[ContPy(), ContC()]), type_equal_sign])
add_csc(acmd)


Grammar for spoken forms
------------------------

Instead of having to encode all the spoken forms of a command from
scratch, it would be better to allow grammar rules instead.

For example instead of:

spoken_forms=['after ;', 'after semi', 'after semicolon', 'goto semi', 'goto semicolon', 'goto ;', 'go semi', 'go semicolon', 'go ;', 'go after semi', 'go after semicolon', 'go after ;']

we could have:

spoken_forms=['after* (semi|semicolon|;)', 'go after* (semi|semicolon|;)']


Symbol translation
------------------

-- Alain

  When a word in a symbol doesn't correspond to an in-vocab word or a
  known abbreviation, we should see if it's a concatenation or such
  words and/or abbreviations.

  
What can I say
--------------

-- Alain

  The mediator console should allow the user to browse through the
  list of CSCs and LSAs by language and topic.

  The user could even ask to see what LSAs and CSCs are applicable in
  the current context (actually, CSCs should be sufficient since a CSC
  is generated automatically for each LSA).

  This feature would use the _doc_ attributes of the Context objects
  and the action functions.

  Example of subject hierarchy for what can I say

  C
    - loops
       for 
       while
       repeat
       etc...
    - conditionals
       if
       then
       else
       switch
       etc...
    - functions
       declaring
       calling
    - precompiler directives
       #include
       etc...
    etc...
   Python
    - loops
        for
    etc...
       
    
Abbreviations cleanup
---------------------

The console should have an abbrev_cleanup command. This would remove
all abbrevs that are not currently used in at least one known symbol.

That way, the user could compile a list of source files he currently
cares about and run clean_abbrevs, thus removing abbreviations that
were generated from old projects that he doesn't care about anymore.

The command would display a list of abbrevs to be removed, giving the
user a chance to keep some of them if he wanted.


Abbreviations definition
------------------------

After compiling a series of source, system could sort abbrevs in
decreasing order of their frequency, and then start a dialog allowing
the user to dictate expansions for them.

This would be useful for batch definition of abbrevs instead of the
automatic as you go definition.


SymDict
-------

-- Alain

Right now, known symbols are removed from NatSpeak's vocabulary every
time VoiceCode exits. This means NatSpeak looses all adaptation it may
do to those symbols.

It would be better to add them with spoken form x_y_z where x, y , z
are words and leave them in the vocab all the time. That way, the user could still erase symbols relatively easily by listing all the user added symbols and removing the ones with _ in their spoken forms.

Of course, this assumes that DNS will recognise x_y_z when you say "x
y z" (i.e. tha the _s won't interfere with recognition).

-- Alain

format_as_symbol
Right now, the order in which the various formats are shown depends on a fixed
rank specified by the programmer.

It would be better if the rank was dynamic, i.e. it was based on how
often the user recently accepted a new symbol formatted in that way in
the current language.


Symbol Compilation
------------------

-- Alain

Python standard libraries have lots of symbols of the form: getenv
which can't be split into their constituents. So when expand_word
finds a word that's neither in-vocab or known abbreviation, it should
try to expand it as a concatenation of abbreviations and in-vocab
words.


Incompatibilities with pkled SymDict
------------------------------------

If you modify the attributes of SymDict, you can end up in a situation
where the pickled SymDict is missing some attributes that the new
SymDict class is expecting. This can result in crashes when users
upgrade VoiceCode.

To deal with this, we should do some kind of compatibility check
between the SymDict class and the pickled instance. This could be
something like:

1. create a new SymDict from scratch.
2. unpickle the old SymDict as old_symbols.
3. retrieve the symbols from old_symbols and add them to the new
dictionary (with add_symbol)
4. ditto for old abbreviations
5. if at any point during this copy process we encounter an exception, just ignore the pickled instance and start with a brand new blank SymDict (make sure you rename the old pickle file in case the exception was not due to an incompatibility... that way, he will not loose his pickled dictionary whenever there is a bug in the system).


